{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOwo-FD93pXw"
   },
   "source": [
    "# **1.0.4 Replicating NLTK Ch. 6**\n",
    "\n",
    "We jump into the NLTK book, chapter 6, the sections 6.1.5 Exploiting context and 6.1.6 Sequence\n",
    "classification. You are advised to read them before you start.\n",
    "We start by importing NLTK and the tagged sentences from the news-section from Brown, similarly\n",
    "to the NLTK book.\n",
    "Then we split the set of sentences into a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQqElHac3d_L",
    "outputId": "a4606cc0-9b39-4566-cfd4-e6239ae00a64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7U1B10Hc4CKC"
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "  features = {\n",
    "      \"suffix(1)\": sentence[i][-1:],\n",
    "      \"suffix(2)\": sentence[i][-2:],\n",
    "      \"suffix(3)\": sentence[i][-3:]\n",
    "  }\n",
    "  if i == 0:\n",
    "    features[\"prev-word\"] = \"<START>\"\n",
    "  else:\n",
    "    features[\"prev-word\"] = sentence[i-1]\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEd27S_h4PZN"
   },
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "  def __init__(self, train_sents, features=pos_features):\n",
    "    self.features = features\n",
    "    train_set = []\n",
    "\n",
    "    for tagged_sent in train_sents:\n",
    "      untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "      history = []\n",
    "\n",
    "      for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featureset = features(untagged_sent, i, history)\n",
    "        train_set.append( (featureset, tag) )\n",
    "        history.append(tag)\n",
    "\n",
    "    self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "  def tag(self, sentence):\n",
    "    history = []\n",
    "    for i, word in enumerate(sentence):\n",
    "      featureset = self.features(sentence, i, history)\n",
    "      tag = self.classifier.classify(featureset)\n",
    "      history.append(tag)\n",
    "    return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRVmc85i4jIs",
    "outputId": "3cd48e88-c00f-4149-a2f6-42f39f2a9436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.accuracy(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFSaIo8p4xaM"
   },
   "source": [
    "## **1.1 Ex 1: Tag set and baseline (10 points)**\n",
    "### **1.1.1 Part a. Tag set and experimental set-up**\n",
    "We will simplify and use the **universal pos tagset** in this exercise. The main reason is that it makes the experiments run faster. With more time, it would have been interesting to also run all the experiments with the original set of tags and compare the effect the tag set has on the results.\n",
    "\n",
    "We will be a little more cautious than the NLTK-book when it comes to training and test sets. \n",
    "We will split the **News-section into** three sets\n",
    "\n",
    "• **10% for final testing which we tuck aside for now, call it news_test**\n",
    "\n",
    "• **10% for development testing, call it news_dev_test**\n",
    "\n",
    "• **80% for training, call it news_train**\n",
    "\n",
    "• **Make the data sets, and repeat the training and evaluation with news_train and news_dev_test.**\n",
    "\n",
    "• **Please use 4 counting decimal places and stick to that throughout the exercise set.**\n",
    "\n",
    "**How is the result compared to using the full brown tagset in the introduction? Why do you think one of the tagsets yields higher scores than the other one?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gOW51Dig4mAP"
   },
   "outputs": [],
   "source": [
    "size = int(len(tagged_sents) * 0.1)\n",
    "tagged_sents_universial = brown.tagged_sents(categories='news', tagset=\"universal\")\n",
    "train_dev, news_test = tagged_sents_universial[size:], tagged_sents_universial[:size]\n",
    "news_train, news_dev_test = tagged_sents_universial[size:], tagged_sents_universial[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bsx1eVNDFOHv",
    "outputId": "4e5e4176-d5fe-4560-eae1-76f937e8f6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873\n"
     ]
    }
   ],
   "source": [
    "tagger_uni = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_uni.accuracy(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIjOWbBTRJdY"
   },
   "source": [
    "The results are stronger when I train and test a model on the smaller tagset. This is because it's easier to distinguish between fewer classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knoa553qGxao"
   },
   "source": [
    "### **1.1.2 Part b. Baseline**\n",
    "\n",
    "One of the first things we should do in an experiment like this, is to establish a reasonable baseline. A reasonable baseline here is the Most Frequent Class baseline. Each word which is seen during training should get its most frequent tag from the training. For words not seen during training, we\n",
    "simply use the most frequent overall tag.\n",
    "\n",
    "With news_train as training set and news_dev_set as valuation set, what is the accuracy of this baseline?\n",
    "\n",
    "Does the tagger from (Part a) using the features from the NLTK book together with the universal tags beat the baseline?\n",
    "\n",
    "Deliveries: Code and results of runs for both parts. For both parts, also answers to the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smAnPIX6Fg1m"
   },
   "outputs": [],
   "source": [
    "class MostFreqClassBaseline(nltk.TaggerI):\n",
    "  def __init__(self, train_data) -> None:\n",
    "    self.class_count = {} # {word -> {tag1 : x1, tag2 : x2, ...}} \n",
    "    self.most_freq_tag_mapper = {} # for unknown words\n",
    "\n",
    "    for sentence in train_data:\n",
    "      for word, tag in sentence:\n",
    "        # add all tags to most_freq_tag\n",
    "        if tag in self.most_freq_tag_mapper:\n",
    "          self.most_freq_tag_mapper[tag] += 1\n",
    "        else:\n",
    "          self.most_freq_tag_mapper[tag] = 1\n",
    "\n",
    "        if word in self.class_count:\n",
    "          if tag in self.class_count[word]:\n",
    "            # +1 for this tag\n",
    "            self.class_count[word][tag] += 1\n",
    "          else:\n",
    "            # add new tag to word \n",
    "            self.class_count[word][tag] = 1\n",
    "        else:\n",
    "          # new word and tag \n",
    "          self.class_count[word] = {tag : 1}\n",
    "\n",
    "    for word in self.class_count:\n",
    "      self.class_count[word] = max(\n",
    "          self.class_count[word], key=self.class_count[word].get\n",
    "      )\n",
    "\n",
    "    self.most_freq_tag = max(self.most_freq_tag_mapper, \n",
    "                             key=self.most_freq_tag_mapper.get)\n",
    "\n",
    "  def tag(self, sentence):\n",
    "    history = []\n",
    "    for word in sentence:\n",
    "      try:\n",
    "        tag = self.class_count[word]\n",
    "      except KeyError:\n",
    "        tag = self.most_freq_tag\n",
    "      history.append(tag)\n",
    "    return zip(sentence, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbVAgFxJH_fO",
    "outputId": "31ee88c3-3ed0-49d5-d4ac-241ce00cff31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9312\n"
     ]
    }
   ],
   "source": [
    "baseline = MostFreqClassBaseline(news_train)\n",
    "print(round(baseline.accuracy(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmDYGC3pRp0j"
   },
   "source": [
    "The tagger from part (a) doesn't beat the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y0N2RIXOHPL"
   },
   "source": [
    "### **1.2 Ex 2: scikit-learn and tuning (10 points)**\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further\n",
    "experiments, we move to scikit-learn which yields more options for considering various alternatives.\n",
    "We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have\n",
    "made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNBclassifier\n",
    "which should correspond to the way it is done in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "D68H_Bt5NcwW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa80Srf8OLdW"
   },
   "outputs": [],
   "source": [
    "class ScikitConsecutivePosTagger(nltk.TaggerI):\n",
    "  def __init__(self, train_sents,\n",
    "    features=pos_features, clf = BernoulliNB()):\n",
    "    # Using pos_features as default.\n",
    "    self.features = features\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for tagged_sent in train_sents:\n",
    "      history = []\n",
    "      untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "      for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featureset = features(untagged_sent, i, history)\n",
    "        train_features.append(featureset)\n",
    "        train_labels.append(tag)\n",
    "        history.append(tag)\n",
    "    v = DictVectorizer()\n",
    "    X_train = v.fit_transform(train_features)\n",
    "    y_train = np.array(train_labels)\n",
    "    clf.fit(X_train, y_train)\n",
    "    self.classifier = clf\n",
    "    self.dict = v\n",
    "  \n",
    "  def tag(self, sentence):\n",
    "    test_features = []\n",
    "    history = []\n",
    "    for i, word in enumerate(sentence):\n",
    "      featureset = self.features(sentence, i, history)\n",
    "      test_features.append(featureset)\n",
    "    X_test = self.dict.transform(test_features)\n",
    "    tags = self.classifier.predict(X_test)\n",
    "    return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puBcuQlPTkiP"
   },
   "source": [
    "#### **1.2.1 Part a.**\n",
    "Train the ScikitConsecutivePosTagger on the news_train set and test on the news_dev_test set\n",
    "with the pos_features. Do you get the same result as with the same data and features and the\n",
    "NLTK code in exercise 1a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9a01RYfOi0E",
    "outputId": "0e04e5da-e181-4383-e798-8f22ff53cce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8635\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(tagger_scikit.accuracy(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DZlTvWCSGHl"
   },
   "source": [
    "This model is alot stronger than the model from 1a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2nFHxoQTdFq"
   },
   "source": [
    "#### **1.2.2 Part b.**\n",
    "\n",
    "I get inferior results compared to using the NLTK set-up with the same feature extractors. The only\n",
    "explanation I could find is that the smoothing is too strong. BernoulliNB() from scikit-learn uses\n",
    "Laplace smoothing as default (“add-one”). The smoothing is generalized to Lidstone smoothing\n",
    "5\n",
    "which is expressed by the alpha parameter to BernoulliNB(alpha=…) Therefore, try again with\n",
    "alpha in [1, 0.5, 0.1, 0.01, 0.001, 0.0001]. What do you find to be the best value for alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA6RojGgQrYt",
    "outputId": "799693e8-4d4b-4251-f46c-74f37cb478b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1 | accuracy = 0.8635\n",
      "alpha = 0.5 | accuracy = 0.8796\n",
      "alpha = 0.1 | accuracy = 0.8754\n",
      "alpha = 0.01 | accuracy = 0.8721\n",
      "alpha = 0.001 | accuracy = 0.8703\n",
      "alpha = 0.0001 | accuracy = 0.8694\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1, 0.5, 0.1, 0.01, 0.001, 0.0001]:\n",
    "  tagger_scikit = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = BernoulliNB(alpha = alpha))\n",
    "  acc = round(tagger_scikit.accuracy(news_dev_test), 4)\n",
    "  print(f\"alpha = {alpha} | accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdWzuwIySPGw"
   },
   "source": [
    "The alpha that yields the best result on the dev set is alpha = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GN0-HNaT0g-"
   },
   "source": [
    "#### **1.2.3 Part c.**\n",
    "\n",
    "To improve the results, we may change the feature selector or the machine learner. We start with\n",
    "a simple improvement of the feature selector. The NLTK selector considers the previous word, but\n",
    "not the word itself. Intuitively, the word itself should be a stronger feature. Extend the NLTK\n",
    "feature selector with a feature for the token to be tagged. Rerun the experiment with various alphas\n",
    "and record the results. Which alpha gives the best accuracy and what is the accuracy?\n",
    "Did the extended feature selector beat the baseline? Intuitively, it should get as least as good\n",
    "accuracy as the baseline. Explain why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmWiANuiVOL2"
   },
   "outputs": [],
   "source": [
    "def pos_features_with_word(sentence, i, history):\n",
    "  features = {\n",
    "      \"suffix(1)\": sentence[i][-1:],\n",
    "      \"suffix(2)\": sentence[i][-2:],\n",
    "      \"suffix(3)\": sentence[i][-3:],\n",
    "      \"word\" : sentence[i]\n",
    "  }\n",
    "  if i == 0:\n",
    "    features[\"prev-word\"] = \"<START>\"\n",
    "  else:\n",
    "    features[\"prev-word\"] = sentence[i-1]\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DisolhyuS7g_",
    "outputId": "6929ad16-49f5-40f9-c593-c0a1e5040fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1 | accuracy = 0.8957\n",
      "alpha = 0.5 | accuracy = 0.9214\n",
      "alpha = 0.1 | accuracy = 0.9288\n",
      "alpha = 0.01 | accuracy = 0.9346\n",
      "alpha = 0.001 | accuracy = 0.9382\n",
      "alpha = 0.0001 | accuracy = 0.9386\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1, 0.5, 0.1, 0.01, 0.001, 0.0001]:\n",
    "  tagger_scikit = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = BernoulliNB(alpha = alpha),\n",
    "      features = pos_features_with_word)\n",
    "  acc = round(tagger_scikit.accuracy(news_dev_test), 4)\n",
    "  print(f\"alpha = {alpha} | accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5RGreDPScP-"
   },
   "source": [
    "The best results are achieved when using an alpha = 0.0001, with an accuracy of 0.94. The model with the extended feature set got a slightly better result compared to the baseline. This is expected because the model should atleast be able to learn what tag is most frequent when the word itself is used as a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn8mkrmnVpih"
   },
   "source": [
    "### **1.3 Ex 3: Logistic regression (10 points)**\n",
    "#### **1.3.1 Part a.**\n",
    "\n",
    "We proceed with the best feature selector from the last exercise. We will study the effect of the\n",
    "learner. Import LogisticRegression and use it with standard settings instead of BernoulliNB. Train\n",
    "on news_train and test on news_dev_test and record the result. Is it better than the best result\n",
    "with Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VekE6SjVkc1",
    "outputId": "67e58217-8a6e-4df5-dd64-f3c102306266"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: accuracy = 0.956\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = LogisticRegression(),\n",
    "      features = pos_features_with_word)\n",
    "acc = round(tagger_scikit.accuracy(news_dev_test), 4)\n",
    "print(f\"LogReg: accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtLIyXMhTU33"
   },
   "source": [
    "The result is alot better with LogisicRegression compared to NaiveBayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0H0SOA_uV3e0"
   },
   "source": [
    "#### **1.3.2 Part b.**\n",
    "Similarly to the Naive Bayes classifier, we will study the effect of smoothing. Smoothing for LogisticRegression is done by regularization. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing. (C is the inverse of the parameter 𝛼 in the lectures.) Try with C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value which yields the\n",
    "best result.\n",
    "\n",
    "Which C gives the best result?\n",
    "\n",
    "Deliveries: Code. Results of the runs. Answers to the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eP4WGZFCV94q",
    "outputId": "d7333d46-14ff-4fb3-a41b-239eccd18ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01 | accuracy = 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1 | accuracy = 0.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1.0 | accuracy = 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10.0 | accuracy = 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 100.0 | accuracy = 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1000.0 | accuracy = 0.956\n"
     ]
    }
   ],
   "source": [
    "for C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "  tagger_scikit = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = LogisticRegression(C = C),\n",
    "      features = pos_features_with_word)\n",
    "  acc = round(tagger_scikit.accuracy(news_dev_test), 4)\n",
    "  print(f\"C = {C} | accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnZa-XBsTi6q"
   },
   "source": [
    "The best results are achieved when C = 10.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsqnY6syXf5C"
   },
   "source": [
    "### **1.4 Ex 4: Features (15 points)**\n",
    "#### **1.4.1 Part a.**\n",
    "\n",
    "We will now stick to the LogisticRegression() with the optimal C from the last point and see whether we are able to improve the results further by extending the feature extractor with more features. First, try adding a feature for the next word in the sentence, and then train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWmKYWWqXb9i"
   },
   "outputs": [],
   "source": [
    "def feature_two_words(sentence, i, history):\n",
    "  features = {\n",
    "      \"suffix(1)\": sentence[i][-1:],\n",
    "      \"suffix(2)\": sentence[i][-2:],\n",
    "      \"suffix(3)\": sentence[i][-3:],\n",
    "      \"word\" : sentence[i],\n",
    "  }\n",
    "  if i == 0:\n",
    "    features[\"prev-word\"] = \"<START>\"\n",
    "  else:\n",
    "    features[\"prev-word\"] = sentence[i-1]\n",
    "\n",
    "  if i == len(sentence) - 1:\n",
    "    features[\"next_word\"] = \"<EOS>\"\n",
    "  else:\n",
    "    features[\"next_word\"] = sentence[i+1]\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Fvn4sFWgS92",
    "outputId": "42b67953-60d9-4f9c-9d41-81d50c92f354"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: accuracy = 0.9672\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = LogisticRegression(C=100),\n",
    "      features = feature_two_words)\n",
    "acc = round(tagger_scikit.accuracy(news_dev_test), 4)\n",
    "print(f\"LogReg: accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh59b8npYgzw"
   },
   "source": [
    "#### **1.4.2 Part b.**\n",
    "\n",
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may consider. Some candidates: is the word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? Consider larger contexts? etc. What is the best feature set you can come up with? Train and test various feature sets and select the best one.\n",
    "\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make references to the sources and explain what you got from them.\n",
    "\n",
    "Observe that the way ScikitConsecutivePosTagger.tag() is written, it extracts the features from a whole sentence before it tags it. Hence it does not support preceding tags as features. It is possible to rewrite ScikitConsecutivePosTagger.tag() to extract features after reading each word, and to use the history which keeps the preceding tags in the sentence. If you like, you may try it. Expect, however, that the tagger will become much slower. We got surprisingly little gain from including preceding tags as features, and you are not requested to trying it.\n",
    "\n",
    "Deliveries: Code. Results of the runs. Answers to the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jcNdAKRYbCj"
   },
   "outputs": [],
   "source": [
    "def feature_set(sentence, i, history, puncts = [ \".\", \",\", \"!\", \"?\", \";\", \":\", \"(\", \")\", \"``\"]):\n",
    "  features = {\n",
    "      \"suffix(1)\": sentence[i][-1:],\n",
    "      \"suffix(2)\": sentence[i][-2:],\n",
    "      \"suffix(3)\": sentence[i][-3:],\n",
    "      \"word\" : sentence[i],\n",
    "      \"word_lower\" : sentence[i].lower(),\n",
    "      \"capitalized\" : sentence[i][0].isupper(),\n",
    "      \"numeric\" : sentence[i].isnumeric(),\n",
    "      \"punct\" : sentence[i] in puncts,\n",
    "      \"contains_hyphen\" : \"-\" in sentence[i]\n",
    "  }\n",
    "  if i == 0:\n",
    "    features[\"prev-word\"] = \"<START>\"\n",
    "  else:\n",
    "    features[\"prev-word\"] = sentence[i-1]\n",
    "    features[\"prev_word_lower\"] : sentence[i-1].lower()\n",
    "    features[\"prev_capitalized\"] = sentence[i-1][0].isupper()\n",
    "    features[\"prev_numeric\"] = sentence[i-1].isnumeric()\n",
    "    features[\"prev_punct\"] = sentence[i-1] in puncts\n",
    "    features[\"prev_contains_hyphen\"] = \"-\" in sentence[i-1]\n",
    "\n",
    "  if i == len(sentence) - 1:\n",
    "    features[\"next_word\"] = \"EOS\"\n",
    "  else:\n",
    "    features[\"next_word\"] = sentence[i+1]\n",
    "    features[\"next_word_lower\"] : sentence[i+1].lower()\n",
    "    features[\"next_capitalized\"] = sentence[i+1][0].isupper()\n",
    "    features[\"next_numeric\"] = sentence[i+1].isnumeric()\n",
    "    features[\"next_punct\"] = sentence[i+1] in puncts\n",
    "    features[\"next_contains_hyphen\"] = \"-\" in sentence[i + 1]\n",
    "  return features\n",
    "\n",
    "# https://towardsdatascience.com/pos-tagging-using-crfs-ea430c5fb78b\n",
    "# got: word_lower and punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbQPSHnFgU6o",
    "outputId": "a43d5818-e0d5-4af0-ccc5-e63f54f708b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: accuracy = 0.9691\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit2 = ScikitConsecutivePosTagger(\n",
    "      news_train, clf = LogisticRegression(),\n",
    "      features = feature_set)\n",
    "\n",
    "acc = round(tagger_scikit2.accuracy(news_dev_test), 4)\n",
    "print(f\"LogReg: accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1D85-F22HxF"
   },
   "source": [
    "### **1.5 Ex 5: Training on a larger corpus (15 points)**\n",
    "#### **1.5.1 Part a.**\n",
    "We have so far used a smaller corpus, the news section, for finding optimal settings for a tagger. We will now try to make a better tagger by training on a larger corpus. We will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: adventure and hobbies. We will also initially stay clear of news to be sure not to mix training and test data.\n",
    "\n",
    "Call the Brown corpus with all categories except these three for rest. Shuffle the tagged sentences from rest and remember to use the universal pos tagset. Then split the set into 80%-10%-10%: *rest_train, rest_dev_test, rest_test*.\n",
    "\n",
    "We can then merge these three sets with the corresponding sets from news to get final training and test sets:\n",
    "\n",
    "• **train = rest_train+news_train**\n",
    "\n",
    "• **dev_test = rest_dev_test + uni_news_dev_test**\n",
    "\n",
    "• **test = rest_test + news_test**\n",
    "\n",
    "Prepare the corpus as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "8bAduGKXwfDu"
   },
   "outputs": [],
   "source": [
    "cats4train = [\n",
    "    # 'adventure',\n",
    "    'belles_lettres',\n",
    "    'editorial',\n",
    "    'fiction',\n",
    "    'government',\n",
    "    # 'hobbies',\n",
    "    'humor',\n",
    "    'learned',\n",
    "    'lore',\n",
    "    'mystery',\n",
    "    # 'news',\n",
    "    'religion',\n",
    "    'reviews',\n",
    "    'romance',\n",
    "    'science_fiction'\n",
    "]\n",
    "\n",
    "rest = [sent for sent in brown.tagged_sents(categories=cats4train, tagset=\"universal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GWAhxF9v361i"
   },
   "outputs": [],
   "source": [
    "# inplace shuffle\n",
    "np.random.shuffle(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sjCdZUBc4EGT"
   },
   "outputs": [],
   "source": [
    "idx = int(len(rest) * 0.1)\n",
    "rest_train_dev, rest_test = rest[idx:], rest[:idx]\n",
    "rest_train, rest_dev = rest_train_dev[idx:], rest_train_dev[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ye84-Wvq8ToZ"
   },
   "outputs": [],
   "source": [
    "test = rest_test + news_test\n",
    "train = rest_train + news_train\n",
    "dev = rest_dev + news_dev_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DulCO6c089wB"
   },
   "source": [
    "#### **1.5.2 Part b.**\n",
    "The next step is to establish a baseline for a tagger trained on this larger corpus, and evaluate it on dev_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq7tbKrd85w4",
    "outputId": "37c76cf7-2731-422e-9b9f-67c4661075a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453\n"
     ]
    }
   ],
   "source": [
    "baseline = MostFreqClassBaseline(train)\n",
    "print(round(baseline.accuracy(dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA7QhZk-9CPv"
   },
   "source": [
    "#### **1.5.3 Part c.**\n",
    "We can then train our tagger on this larger corpus. Use the best settings from the earlier exercises, train on train and test on dev_test. What is the accuracy of your tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_gdrrw59FDs",
    "outputId": "d5df1c4a-1fdf-415a-a466-c08136969bb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: accuracy = 0.9764\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit_entire_brown = ScikitConsecutivePosTagger(\n",
    "      train, \n",
    "      clf = LogisticRegression(\n",
    "          #C=100, \n",
    "          # solver=\"saga\", \n",
    "          max_iter = 100,\n",
    "          # penalty = \"none\"\n",
    "      ),\n",
    "      features = feature_set)\n",
    "\n",
    "acc = round(tagger_scikit_entire_brown.accuracy(dev), 4)\n",
    "print(f\"LogReg: accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD6jxjDMUD1L"
   },
   "source": [
    "The accuracy is 0.98. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FWfunI0QJil"
   },
   "source": [
    "###**1.6 Ex6: Evaluation metrics (10 points)**\n",
    "####**1.6.1 Part a.**\n",
    "The accuracy should be quite decent now > 0.97. Still, we will like to find out more about where the tagger makes mistakes. With only 12 different tags, we can get all the results into a confusion table. Take a look at https://www.nltk.org/api/nltk.tag.api.html and make a confusion table for the results on dev_test.\n",
    "Make sure you understand what the rows and columns are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KBXqKNOHaMH"
   },
   "outputs": [],
   "source": [
    "conf_mat = tagger_scikit_entire_brown.confusion(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwQxNyHDSC5h",
    "outputId": "f1255c15-f442-4179-a1b2-12e2910c4408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . |<12701>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <6908>    4   170     .     .   256     3     .     .    90     . |\n",
      " ADP |     1     3<12535>   45     5    19     5     1     5   168    10     . |\n",
      " ADV |     .   156    84 <4457>    7    19    34     .     .    33     6     . |\n",
      "CONJ |     .     .     4     5 <3143>    1     .     .     .     .     1     . |\n",
      " DET |     .     .    23     6     1<11989>    2     .    24     .     .     . |\n",
      "NOUN |     .   255     .    17     .     2<23837>   39     2     2   223     3 |\n",
      " NUM |     .     3     .     .     .     .    19 <1237>    .     .     .     . |\n",
      "PRON |     .     .    44     1     .    22     5     . <4169>    .     .     . |\n",
      " PRT |     .     3    94    23     .     .    24     .     1 <2428>    8     . |\n",
      "VERB |     .    34    10    14     .     .   272     .     1     .<15562>    . |\n",
      "   X |     .     6     .     1     .     .    68     .     1     .     3   <19>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV2kkR0dQsDS"
   },
   "source": [
    "####**1.6.2 Part b.**\n",
    "Finding hints on the same web page, calculate the precision, recall and f-measure for each tag and display the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PnL76P0aXjtK"
   },
   "outputs": [],
   "source": [
    "def print_table(metric, metric_dict):\n",
    "  print(metric)\n",
    "  print(\"-\"*13)\n",
    "  for k,v in metric_dict.items():\n",
    "    print(f\"{k:<4}\", \":\", str(round(v,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3tLoFmgQnN_"
   },
   "outputs": [],
   "source": [
    "prec = tagger_scikit_entire_brown.precision(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfIAJf1tUqYF",
    "outputId": "a525ffab-74f2-4733-88ac-bc4d15146a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision\n",
      "-------------\n",
      ".    : 0.9999\n",
      "ADJ  : 0.9376\n",
      "ADP  : 0.9794\n",
      "ADV  : 0.9405\n",
      "CONJ : 0.9959\n",
      "DET  : 0.9948\n",
      "NOUN : 0.9721\n",
      "NUM  : 0.9664\n",
      "PRON : 0.9919\n",
      "PRT  : 0.9228\n",
      "VERB : 0.9786\n",
      "X    : 0.8636\n"
     ]
    }
   ],
   "source": [
    "print_table(\"precision\", prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c92HusDIRzei"
   },
   "outputs": [],
   "source": [
    "recall = tagger_scikit_entire_brown.recall(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cn-k407PW1eE",
    "outputId": "40b352f4-8e45-4e6b-d447-b01af939411a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall\n",
      "-------------\n",
      ".    : 1.0\n",
      "ADJ  : 0.9296\n",
      "ADP  : 0.9795\n",
      "ADV  : 0.9293\n",
      "CONJ : 0.9965\n",
      "DET  : 0.9954\n",
      "NOUN : 0.9777\n",
      "NUM  : 0.9825\n",
      "PRON : 0.983\n",
      "PRT  : 0.9407\n",
      "VERB : 0.9792\n",
      "X    : 0.1939\n"
     ]
    }
   ],
   "source": [
    "print_table(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgybEiqARz7W"
   },
   "outputs": [],
   "source": [
    "f_measure = tagger_scikit_entire_brown.f_measure(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuWn30aJXUdU",
    "outputId": "6a49e423-895b-442a-80e5-a0318f6968c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_measure\n",
      "-------------\n",
      ".    : 1.0\n",
      "ADJ  : 0.9336\n",
      "ADP  : 0.9795\n",
      "ADV  : 0.9349\n",
      "CONJ : 0.9962\n",
      "DET  : 0.9951\n",
      "NOUN : 0.9749\n",
      "NUM  : 0.9744\n",
      "PRON : 0.9874\n",
      "PRT  : 0.9317\n",
      "VERB : 0.9789\n",
      "X    : 0.3167\n"
     ]
    }
   ],
   "source": [
    "print_table(\"f_measure\", f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1umTgA6Qw5l"
   },
   "source": [
    "####**1.6.3 Part c.**\n",
    "Calculate the macro precision, macro recall and macro f-measure across the 12 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bq6csAt4QzXB",
    "outputId": "d0e86fec-62ce-46fc-aacf-db6eb6102235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision: 0.962\n"
     ]
    }
   ],
   "source": [
    "# macro precision:\n",
    "macro_prec = sum(prec.values())/12\n",
    "print(f\"Macro precision: {macro_prec:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZl30EpKUxJR",
    "outputId": "0fca2026-e0b3-46e4-89bf-c342d7aa9978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro recall: 0.907\n"
     ]
    }
   ],
   "source": [
    "# macro precision:\n",
    "macro_recall = sum(recall.values())/12\n",
    "print(f\"Macro recall: {macro_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7-yERoxU8DG",
    "outputId": "890c7b6b-1d86-40d9-e100-d906e6fc0ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro f-measure: 0.917\n"
     ]
    }
   ],
   "source": [
    "# macro precision:\n",
    "macro_f = sum(f_measure.values())/12\n",
    "print(f\"Macro f-measure: {macro_f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfwIT5E-mzEz"
   },
   "source": [
    "### **1.7 Ex 7: Error analysis (10 points)**\n",
    "Sometimes when we make classifiers for NLP phenomena, it makes sense to inspect the errors more thoroughly. Where does the classifier make errors? What kind of errors? Find five sentences where at least one token is mis-classified, and display these sentences on the follwing form, with the pred(icted) and gold tags.\n",
    "\n",
    "Identify the words that are tagged differently. Comment on each of the differences. Would you say that the predicted tag is wrong? Or is there a genuine ambiguity such that both answers are defendable? Or is even the gold tag wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuLnEXW5ZJFB"
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for sent in dev:\n",
    "  sent2tag = [i[0] for i in sent]\n",
    "  tags = [i[1] for i in sent]\n",
    "  sent_preds = list(tagger_scikit_entire_brown.tag(sent2tag))\n",
    "  preds = [i[1] for i in sent_preds]\n",
    "  eq = (preds == tags)\n",
    "  if not eq:\n",
    "    info = (sent, sent_preds)\n",
    "    errors.append(info)\n",
    "  \n",
    "  if len(errors) == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3PODyPJpnsz"
   },
   "outputs": [],
   "source": [
    "def error_parser(error_iter):\n",
    "  print(\"Token          pred   gold\")\n",
    "  print(\"==========================\")\n",
    "  for i in range(len(error_iter[0])):\n",
    "    s = f\"{error_iter[0][i][0]}\".ljust(15)\n",
    "    print(s, end=\"\")\n",
    "    s2 = f\"{error_iter[1][i][1]}\"\n",
    "    s3 = f\"{error_iter[0][i][1]}\"\n",
    "    print(s2.ljust(5), end=\" \")\n",
    "    print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YF-316_p4kW",
    "outputId": "9456e09d-e77c-4800-cb2a-3ec2a9f04c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          pred   gold\n",
      "==========================\n",
      "A              DET   DET\n",
      "seeping        VERB  VERB\n",
      "coldness       NOUN  NOUN\n",
      "entered        VERB  VERB\n",
      "Holden's       NOUN  NOUN\n",
      "being          VERB  NOUN\n",
      ";              .     .\n",
      ";              .     .\n"
     ]
    }
   ],
   "source": [
    "error_iter = iter(errors)\n",
    "error_parser(next(error_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaD_PYkiZLZY"
   },
   "source": [
    "The classifier is definetly wrong, but I consider this to be an honest mistake. \"being\" is usually a verb, but in this context it has been used as a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amkieUtKng_0",
    "outputId": "46d0904a-4839-4021-e1de-28172052560a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          pred   gold\n",
      "==========================\n",
      "Urethane       NOUN  NOUN\n",
      "foams          NOUN  NOUN\n",
      "are            VERB  VERB\n",
      ",              .     .\n",
      "basically      ADV   ADV\n",
      ",              .     .\n",
      "reaction       NOUN  NOUN\n",
      "products       NOUN  NOUN\n",
      "of             ADP   ADP\n",
      "hydroxyl-rich  ADJ   ADJ\n",
      "materials      NOUN  NOUN\n",
      "and            CONJ  CONJ\n",
      "polyisocyanatesNOUN  NOUN\n",
      "(              .     .\n",
      "usually        ADV   ADV\n",
      "tolylene       ADJ   NOUN\n",
      "diisocyanate   NOUN  NOUN\n",
      ")              .     .\n",
      ".              .     .\n"
     ]
    }
   ],
   "source": [
    "error_parser(next(error_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-AAgcfQZjt3"
   },
   "source": [
    "The word that is misclassified is as an ADJ instead og a NOUN is a very rare word. Classifying a seldom word before a noun as a adjective is a safe bet. This time the classifier was wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEKRkCoCnyez",
    "outputId": "28707667-e0cd-4aaa-f0db-45da6037a3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          pred   gold\n",
      "==========================\n",
      "I              PRON  PRON\n",
      "had            VERB  VERB\n",
      "brushed        VERB  VERB\n",
      "my             DET   DET\n",
      "teeth          NOUN  NOUN\n",
      ",              .     .\n",
      "showered       VERB  VERB\n",
      ",              .     .\n",
      "shaved         VERB  VERB\n",
      "and            CONJ  CONJ\n",
      "dressed        VERB  VERB\n",
      "by             ADP   ADP\n",
      "the            DET   DET\n",
      "time           NOUN  NOUN\n",
      "a              DET   DET\n",
      "waiter         NOUN  NOUN\n",
      "wheeled        VERB  VERB\n",
      "in             ADP   PRT\n",
      "breakfast      NOUN  NOUN\n",
      ".              .     .\n"
     ]
    }
   ],
   "source": [
    "error_parser(next(error_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hqutks6aYkG"
   },
   "source": [
    "The word \"in\" was misclassified as ADP instead of PRT. The classifier is wrong, but I get why. I would also probably tag this as a ADP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCU7z90tnzlL",
    "outputId": "ecb71ccd-7431-46ec-9f3c-0304a023e479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          pred   gold\n",
      "==========================\n",
      "The            DET   DET\n",
      "poet's         NOUN  NOUN\n",
      "intentions     NOUN  NOUN\n",
      "are            VERB  VERB\n",
      "difficult      ADJ   ADJ\n",
      "to             PRT   PRT\n",
      "discern        VERB  VERB\n",
      "and            CONJ  CONJ\n",
      ",              .     .\n",
      "except         ADP   ADP\n",
      "to             PRT   ADP\n",
      "biographers    NOUN  NOUN\n",
      ",              .     .\n",
      "unimportant    ADJ   ADJ\n",
      ";              .     .\n",
      ";              .     .\n"
     ]
    }
   ],
   "source": [
    "error_parser(next(error_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1FiBQvKcjla"
   },
   "source": [
    "The classifier seems to struggle with the differene between particles and adpositions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8j-sR_movbw",
    "outputId": "2114b9e3-a011-45d3-d544-a2aa45165c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          pred   gold\n",
      "==========================\n",
      "This           DET   DET\n",
      "was            VERB  VERB\n",
      "the            DET   DET\n",
      "very           ADV   ADJ\n",
      "sort           NOUN  NOUN\n",
      "of             ADP   ADP\n",
      "legislation    NOUN  NOUN\n",
      "that           ADP   PRON\n",
      "Roosevelt      NOUN  NOUN\n",
      "himself        PRON  PRON\n",
      "had            VERB  VERB\n",
      "in             ADP   ADP\n",
      "mind           NOUN  NOUN\n",
      ".              .     .\n"
     ]
    }
   ],
   "source": [
    "error_parser(next(error_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YTfkcuKc9Os"
   },
   "source": [
    "The classifier misclassified the word \"very\" as an adverb. It must have mistaken the context, and interpreted the next word sort. The classifier most likely the next word was the verb to sort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks9xElxmxBI3"
   },
   "source": [
    "####**1.8 Ex 8: Final testing (10 points)**\n",
    "###**1.8.1 Part a.**\n",
    "We have reached a stage where we will make no further adjustments to our tagger. We are ready to perform the final testing. First, test the final tagger from exercise 5 on the the test set test?\n",
    "\n",
    "How is the result compared to dev_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKScvSIZvXfT",
    "outputId": "90d6c6b8-0116-47d8-90fb-5854242de371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {tagger_scikit_entire_brown.accuracy(test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehC_1noadV9J"
   },
   "source": [
    "The results from the dev set and the test set are pretty much the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymugc03qxiOP"
   },
   "source": [
    "###**1.8.2 Part b.**\n",
    "We will compare in-domain to out-of-domain testing. Test the big tagger first on adventures then on hobbies. Discuss in a few sentences why you see different results from when testing on test. Why do you think you got different results on adventures compared to hobbies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJo8M-n3xTpw"
   },
   "outputs": [],
   "source": [
    "adventure = [sent for sent in brown.tagged_sents(categories=\"adventure\", tagset=\"universal\")]\n",
    "hobbies = [sent for sent in brown.tagged_sents(categories=\"hobbies\", tagset=\"universal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SGbRraTx-5K",
    "outputId": "0832b7c3-f42b-4545-d0af-c6bc3b99801e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {tagger_scikit_entire_brown.accuracy(adventure):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSBNzaSFyBh5",
    "outputId": "077c0043-835d-420a-a75e-9888421ba613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {tagger_scikit_entire_brown.accuracy(hobbies):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qVWojwQdjrR"
   },
   "source": [
    "The results doesn't differ much. The differences might just be random. But if we were to interpret it, the reason might be hobbies being more different from the categories the model was trained on, compared to adventure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6hWfBCayb49"
   },
   "source": [
    "###**1.9 Ex 9: Comparing to other taggers (10 points)**\n",
    "####**1.9.1 Part a.**\n",
    "In the lectures, we spent quite some time on the HMM-tagger. NLTK comes with an HMM-tagger which we may train and test on our own corpus. It can be trained by\n",
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train) and tested similarly as we have tested our other taggers. Train and test it, first on the news set then on the big train/test set. \n",
    "\n",
    "How does it perform compared to your best tagger? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa1W1b7myDTh",
    "outputId": "8837b8c9-bbea-443e-8c6a-c6e60fbe46dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 25.5 ms, total: 1.07 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB7pid50yqcp",
    "outputId": "717faf31-f967-4218-f2f2-5407b07f6f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9078\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {news_hmm_tagger.accuracy(news_dev_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtExe_mayyNA",
    "outputId": "959030a5-b3ed-41a8-9fde-18472f883f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 s, sys: 28.2 ms, total: 2.96 s\n",
      "Wall time: 2.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_hmm_tagger_entire_brown = nltk.HiddenMarkovModelTagger.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5-l_6NxeOyt",
    "outputId": "dc002632-a4ae-4ead-b886-ff78fba2ad77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {news_hmm_tagger.accuracy(test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWUsqcH8epPe"
   },
   "source": [
    "The accuracy is worse compared to my best tagger, but training is alot faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuQbY8VazC3u"
   },
   "source": [
    "####**1.9.2 Part b**\n",
    "NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently considered the best tagger included with NLTK. It can be trained as follows:\n",
    "\n",
    "%%time\n",
    "\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "\n",
    "per_tagger.train(train)\n",
    "\n",
    "It is tested similarly to our other taggers. Train and test it, first on the news set and then on the big train/test set. How does it perform compared to your best tagger? Did you beat it? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zwz1LCby-kV",
    "outputId": "3c6c0291-0ea0-4603-aad1-2c2905c45530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 420 ms, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1ZoIn3GzRiR",
    "outputId": "bdffe0c9-08ae-4b88-874b-55077fb01801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {per_tagger.accuracy(test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXthB_ojd72K"
   },
   "source": [
    "The perceptron is a bit slower to train, atleast compared to the HMM. But the accuracy on the test set is pretty good, and is on pair with my best tagger. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
